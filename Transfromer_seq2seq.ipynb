{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!python -m spacy download de_core_news_sm\\n!python -m spacy download en_core_web_sm'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import spacy\n",
    "import io\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get English and German Language Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    unk_token = '<unk>'\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"UTF-8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    v = vocab(counter, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    # v.set_default_index(default_index)\n",
    "    v.set_default_index(v[unk_token])\n",
    "    return v\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\"\"\"\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "  unk_token = '<unk>'\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  v = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "  v.set_default_index(v[unk_token])\n",
    "  return v\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19214, 10837)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(de_vocab.get_stoi().values()), max(en_vocab.get_stoi().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "max_seq_len = 100 \n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "print(PAD_IDX, BOS_IDX, EOS_IDX)\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        \n",
    "        pad_de = torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX], dtype=int)], dim=0) #, torch.ones(max_seq_len - (de_item.size()[0]+2))\n",
    "        de_batch.append(pad_de)\n",
    "        \n",
    "        pad_en = torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX], dtype=int)], dim=0) #, torch.ones(max_seq_len - (en_item.size()[0]+2))\n",
    "        en_batch.append(pad_en)\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)  #, batch_first=True\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)  #, batch_first=True\n",
    "    # print(de_batch.size(), en_batch.size())\n",
    "    return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "valid_iter = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "test_iter = DataLoader(test_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Pakete der Pytroch Bibliothek und Numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mask=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask = mask\n",
    "        self.num_heads = num_heads # Anzahl der Attention Heads\n",
    "        self.emb_size = d_model # Dimension der Einbettungen\n",
    "\n",
    "        d_xq = d_xk = d_xv = self.emb_size # Dimension der Einbetung Keys, Queries und Values zuweisen \n",
    "\n",
    "        self.d_head = d_model//num_heads # reduzierte Dimension der einzelnen Attention-Heads\n",
    "\n",
    "        # Zuerst sind die Matrizen noch in der normalen Dimension der Einbettungen und werden noch gesplittet\n",
    "        self.W_Q = nn.Linear(d_xq, d_xq, bias=False) # Gewichtsmatrix Queries\n",
    "        self.W_K = nn.Linear(d_xk, d_xk, bias=False) # Gewichtsmatrix Keys\n",
    "        self.W_V = nn.Linear(d_xv, d_xv, bias=False) # Gewichtsmatrix Values\n",
    "\n",
    "        # Die Ausgabe aller Subschichten müssen der Dimension d_model=emb_size=seq_len entsprechen\n",
    "        self.W_H = nn.Linear(self.emb_size, self.emb_size) #Ursprünglich: self.W_H = nn.Linear(self.emb_size, self.seq_len)'\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        seq_len = Q.size(2)\n",
    "        if mask is None:\n",
    "            scores = torch.matmul(Q, K.transpose(2, 3)) / np.sqrt(self.d_head)\n",
    "        else:\n",
    "            scores = torch.matmul(Q, K.transpose(2, 3)) / np.sqrt(self.d_head)\n",
    "            mask = make_mask(seq_len)\n",
    "            scores = scores + mask\n",
    "\n",
    "        # Reihenweise Anweundung der Softmax-Funktion in der Attention-Matrix\n",
    "        A = nn.Softmax(dim=-1)(scores)\n",
    "        # Gewichten der Values mit der Attention-Matrix\n",
    "        H = torch.matmul(A, V)\n",
    "\n",
    "        return H\n",
    "    \n",
    "    def split_heads(self, x, batch_size, seq_len):\n",
    "        \"\"\" \n",
    "        x_split = x.view(batch_size,self.seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        print(x_split)\n",
    "        return x_split\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"x:\", x.size())\n",
    "        #print(\"x flat\", x.flatten().size())self.seq_len\n",
    "        \n",
    "        x_view = x.view(batch_size, seq_len, self.num_heads, self.d_head) #(batch_size x seq_len x num_heads x head_dim)\n",
    "        return x_view.transpose(1,2)\n",
    "    \n",
    "    def group_heads(self, x, batch_size):\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_head)\n",
    "    \n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size = X_q.size()[0]\n",
    "        Q = self.split_heads(self.W_Q(X_q), batch_size, X_q.size()[1]) # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_K(X_k), batch_size, X_k.size()[1]) # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_V(X_v), batch_size, X_v.size()[1]) # (bs, n_heads, v_length, dim_per_head)\n",
    "\n",
    "        H_head = self.scaled_dot_product_attention(Q, K, V, mask=self.mask)\n",
    "        H_cat = self.group_heads(H_head, batch_size)\n",
    "        \n",
    "        H = self.W_H(H_cat)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des vollvernetzten Netzwerks\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Layer1 = nn.Linear(d_model, d_model*hidden_dim) \n",
    "        self.Layer2 = nn.Linear(d_model*hidden_dim, d_model) \n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.Layer1(x)\n",
    "        h2 = self.Layer2(h1)\n",
    "        out = self.activation(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einbettungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Wort- und Positionseinbettungen \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, src_pad_idx):\n",
    "        super().__init__()\n",
    "        self.word_embedings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=src_pad_idx) \n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.emb_dim = d_model\n",
    "\n",
    "    \n",
    "    def positional_embedding(self, seq_len, emb_size):\n",
    "        pos_array = np.array([\n",
    "            [p/np.power(10000, 2 * (i//2)/emb_size) for i in range(emb_size)]\n",
    "            for p in range(seq_len)])\n",
    "        \n",
    "        pos_array[:, 0::2] = np.sin(pos_array[:, 0::2])\n",
    "        pos_array[:, 1::2] = np.cos(pos_array[:, 1::2])\n",
    "        pos_emb = torch.from_numpy(pos_array)\n",
    "        pos_emb.requires_grad = False\n",
    "        pos_emb = pos_emb.to(device)\n",
    "        #self.register_buffer(\"pos_emb\", pos_emb)\n",
    "        \n",
    "        return pos_emb\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # print(input_ids.size())\n",
    "        seq_len = input_ids.size()[1]\n",
    "        # Zuweisen der Worteinbettungen\n",
    "        word_emb = self.word_embedings(input_ids.long()) * np.sqrt(self.emb_dim)\n",
    "\n",
    "        # Zuweisen der Positionseinbettungen\n",
    "        position_emb = self.positional_embedding(seq_len, self.emb_dim)\n",
    "        # print(\"Dim Encoded Seq: \", word_emb.size())\n",
    "        # print(\"Wordembedding size: \", word_emb.size(), \"Wordembedding size: \",position_emb.size())\n",
    "        # Einbettungen Addieren\n",
    "        # print(\"Word Embeddings: \", word_emb.size(), \"Pos Embeding: \", position_emb.size())\n",
    "        \n",
    "        embeddings = word_emb + position_emb\n",
    "        # print(\"Embeddings done\")\n",
    "        # print(\"Final Embedding DIm: \", embeddings.size())\n",
    "        # Normaliesieren der Einbettungen\n",
    "        embeddings = self.layernorm(embeddings.float())\n",
    "        \n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "\n",
    "seq_len = 100\n",
    "emb_size = 512\n",
    "\n",
    "pos_array = np.array([\n",
    "        [p/np.power(10000, 2 * (i//2)/emb_size) for i in range(emb_size)]\n",
    "        for p in range(seq_len)])\n",
    "    \n",
    "pos_array[:, 0::2] = np.sin(pos_array[:, 0::2])\n",
    "pos_array[:, 1::2] = np.cos(pos_array[:, 1::2])\n",
    "pos_emb = torch.from_numpy(pos_array)\n",
    "pos_emb.requires_grad = False\n",
    "pos_emb = pos_emb.unsqueeze(0)#.to(device)\n",
    "pos_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(seq_len):\n",
    "    mask = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j > i: mask[i][j] = 1\n",
    "    return mask.masked_fill(mask==1, float('-inf')).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen einer Encoder-Schicht\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_h_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(d_model, ff_h_dim)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"X in: \", x.size())\n",
    "        mha_out = self.mha(x, x, x)\n",
    "        # print(\"mha out: \",mha_out.size())\n",
    "        out1 = self.layernorm1(x + mha_out) # x + mha_out = Residuen Verbindung\n",
    "        \n",
    "        ffn_out = self.ffn(out1) \n",
    "        out2 = self.layernorm2(out1 + ffn_out)\n",
    "\n",
    "        return out2\n",
    "\n",
    "# Erstellen des Encoders\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_h_dim, vocab_size, src_pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embeddings(d_model, vocab_size, src_pad_idx)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_h_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Decoder Schicht\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, mask=True)\n",
    "        self.ffn = FFN(d_model, dff)\n",
    "        self.cmha = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm3 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x_dec, x_enc):\n",
    "        mha_out = self.mha(x_dec, x_dec, x_dec)\n",
    "        out1 = self.layernorm1(mha_out + x_dec)\n",
    "        cmha_out = self.cmha(out1, x_enc, x_enc)\n",
    "        out2 = self.layernorm2(cmha_out + out1)\n",
    "        ffn_out = self.ffn(out2)\n",
    "        out3 = self.layernorm3(ffn_out + out2)\n",
    "\n",
    "        return out3 \n",
    "\n",
    "# Erstellen des Encoders\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_h_dim, trg_vocab_size, src_pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "        self.trg_embedding = Embeddings(d_model, trg_vocab_size, src_pad_idx)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.dec_layers.append(DecoderLayer(d_model, num_heads, ff_h_dim))\n",
    "    \n",
    "    def forward(self, x_trg, x_src):\n",
    "        x = self.trg_embedding(x_trg.long())\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, x_src)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_h_dim, src_vocab_size, trg_vocab_size, src_pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, ff_h_dim, src_vocab_size, src_pad_idx)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, ff_h_dim, trg_vocab_size, src_pad_idx)\n",
    "        self.class_layer = nn.Linear(d_model, trg_vocab_size)\n",
    "\n",
    "    def forward(self, x_src, x_trg):\n",
    "        # print(\"Start Encoder\")        \n",
    "        x_enc = self.encoder(x_src)\n",
    "        # print(\"Start Decoder\")\n",
    "        x_dec = self.decoder(x_trg, x_enc)\n",
    "        x = self.class_layer(x_dec)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20838 29215\n"
     ]
    }
   ],
   "source": [
    "# Set Training Parameters\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Set Model Parameters\n",
    "src_vocab_size = len(en_vocab)+10000#max(en_vocab.get_stoi().values()) + 10000 # 30000\n",
    "trg_vocab_size = len(de_vocab)+10000# max(de_vocab.get_stoi().values()) + 2000 # 35000\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "max_len = max_seq_len \n",
    "expansion_factor = 4\n",
    "src_pad_idx = de_vocab.get_stoi()[\"<pad>\"]\n",
    "print(src_vocab_size, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embedings): Embedding(20838, 512, padding_idx=1)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (trg_embedding): Embeddings(\n",
       "      (word_embedings): Embedding(29215, 512, padding_idx=1)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (dec_layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cmha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cmha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (Layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (Layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cmha): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_H): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_layer): Linear(in_features=512, out_features=29215, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstellen eines Pseudo Modells\n",
    "transformer = TransformerEncoderDecoder(num_layers=num_layers,\n",
    "                                    d_model=embedding_size,\n",
    "                                    num_heads=num_heads,\n",
    "                                    ff_h_dim=expansion_factor,\n",
    "                                    src_vocab_size=src_vocab_size,\n",
    "                                    trg_vocab_size=trg_vocab_size, \n",
    "                                    src_pad_idx=src_pad_idx)\n",
    "                                    \n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total Parameter:  62671903\n",
      "Num total trainable Parameter:  62671903\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num total Parameter: \", sum(p.numel() for p in transformer.parameters()))\n",
    "print(f\"Num total trainable Parameter: \", sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr) \n",
    "\n",
    "pad_idx = de_vocab.get_stoi()[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model: nn.Module, src, start_symbol):\n",
    "    seq_len = src.size(1)\n",
    "    src = src.to(device)\n",
    "    out = model.encoder(src)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for _ in range(seq_len-1):\n",
    "        enc_out = out.to(device)\n",
    "        #print(\"Enc out:\", enc_out.size())\n",
    "        dec_out = model.decoder(ys, enc_out)\n",
    "        #print(\"Dec out:\", dec_out.size())\n",
    "        prob = model.class_layer(dec_out[:, -1]) #\n",
    "\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys         \n",
    "\n",
    "def translate(model, src, src_vocab, trg_vocab, src_tokenizer):\n",
    "    model.eval()\n",
    "    tokens = [BOS_IDX] + [src_vocab.get_stoi()[token] for token in src_tokenizer(src)] + [EOS_IDX]\n",
    "    seq_len = len(tokens)\n",
    "    src = (torch.LongTensor(tokens).reshape(1, seq_len))\n",
    "    trg_tokens = greedy_search(model, src, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join([trg_vocab.get_itos()[token] for token in trg_tokens])#.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_data, batch_size):\n",
    "    num_batches = batch_size\n",
    "    eval_acc = 0\n",
    "    for batch in valid_data:\n",
    "        src_lang = batch[0].to(device)\n",
    "        trg_lan = batch[1].to(device)\n",
    "        out = transformer(src_lang, trg_lan[:, 1:])\n",
    "        eval_acc += (out.argmax(1)==trg_lan).cpu().numpy().mean()\n",
    "    \n",
    "    print(f\"Validierungs Accuracy: {eval_acc/num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"We came a long way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 10]\n",
      "=> Saving checkpoint\n",
      "torch.Size([128, 30]) torch.Size([128, 31])\n",
      "Testübersetzung:  <bos> ein ein ein ein ein ein ein\n",
      "Loss of Epoch 0 Batch 0:  10.313152313232422\n",
      "torch.Size([128, 27]) torch.Size([128, 28])\n",
      "Testübersetzung:  <bos> ein ein ein ein ein ein ein\n",
      "Loss of Epoch 0 Batch 1:  9.963177680969238\n",
      "torch.Size([128, 26]) torch.Size([128, 31])\n",
      "Testübersetzung:  <bos> ein ein ein ein ein ein ein\n",
      "Loss of Epoch 0 Batch 2:  9.781220436096191\n",
      "torch.Size([128, 32]) torch.Size([128, 25])\n",
      "Testübersetzung:  <bos> ein ein ein ein ein ein ein\n",
      "Loss of Epoch 0 Batch 3:  9.67807388305664\n",
      "torch.Size([128, 34]) torch.Size([128, 26])\n",
      "Testübersetzung:  <bos> ein ein ein ein ein ein ein\n",
      "Loss of Epoch 0 Batch 4:  9.58344841003418\n",
      "torch.Size([128, 31]) torch.Size([128, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [02:03<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    torch.load(\"model.pt\")\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": transformer.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }\n",
    "        utils.save_checkpoint(checkpoint)\n",
    "        \n",
    "    transformer.train()\n",
    "    losses = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for i, (src, trg) in enumerate(train_iter):\n",
    "        print(src.size(), trg.size())\n",
    "        src, trg = src.to(device), trg.to(device) # (bs x seq_len_en), (bs x seq_len_le)\n",
    "        trg_input = trg[:, :-1]\n",
    "        # print(\" \".join([en_vocab.get_itos()[tok] for tok in trg[1]]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join([de_vocab.get_itos()[tok] for tok in src[1]]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "\n",
    "        output = transformer(src, trg_input) # (bs x trg_seq_len x trg_vocab_size)\n",
    "        \n",
    "        output = output.view(output.size()[0]*output.size()[1],-1) # (bs * trg_seq_len x trg_vocab_size)\n",
    "        target = trg[:, 1:].reshape(-1) # (trg_seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        losses += loss.item()\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += (output.argmax(1) == target).cpu().numpy().mean()/output.size()[0]\n",
    "        translation = translate(transformer, sen, en_vocab, de_vocab, en_tokenizer)\n",
    "        print(f\"Testübersetzung: \", translation)\n",
    "        print(f\"Loss of Epoch {epoch} Batch {i}: \", loss.item())\n",
    "\n",
    "    print(f\"Fehler in der Epoche {epoch} ist {losses / num_epochs}\")\n",
    "    print(f\"Training Accuracy: {train_acc / num_epochs}\")\n",
    "    print('Performance mit Validierungsdaten:')\n",
    "    ## evaluate(valid_data)   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "921a8f3529ec7602b18aed4a6f6291e1e6a6b47e685d252dce53619f5cc88ac3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('NeuralNetworks': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
